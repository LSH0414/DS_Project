{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8894e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent(verify_ssl=False)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import free_proxy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fecd5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://lostark.inven.co.kr/'\n",
    "\n",
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ec14c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = bs(res.text, 'html.parser').select_one('#lostarkBody > div.commu-wrap > section > article > section.commu-left > div.left-menu > ul.list.tablist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cae2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = class_list.select('span > a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b3ac351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open ('./data/class_doe.json', 'w') as f:\n",
    "    json.dump(class_code,f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf00eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_code = dict()\n",
    "class_url = dict()\n",
    "\n",
    "for cl in class_list:\n",
    "    url = cl['href']\n",
    "    base_url = 'https://www.inven.co.kr/board/lostark/'\n",
    "    code = int(re.sub(base_url, '', url))\n",
    "    name = cl.text\n",
    "    \n",
    "    class_code[name] = code # key : 이름, value : code\n",
    "    class_url[code] = url # key : code, value : URL\n",
    "    \n",
    "code_to_name = dict()\n",
    "\n",
    "for key, value in class_code.items():\n",
    "    code_to_name[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf47ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_basic_info(soup):\n",
    "    \n",
    "    num_lst = []\n",
    "    title_lst = []\n",
    "    url_lst = []\n",
    "    \n",
    "    # soup에서 게시글만 가져오기\n",
    "    posts = soup.select('#new-board > form > div > table > tbody > tr')\n",
    "    \n",
    "    for post in posts:\n",
    "        if len(post['class']):\n",
    "            pass\n",
    "        else:\n",
    "\n",
    "            num_lst.append(post.select_one('tr > td > span').text) # 글 번호\n",
    "            url_lst.append(post.select_one('a')['href']) # 글 링크\n",
    "\n",
    "            post_title = (post.select_one('a').text) # 글 제목\n",
    "            post_title = re.sub('[\\n\\t]+','',post_title)\n",
    "            title_lst.append(re.sub('[ ]{2}','',post_title))\n",
    "            \n",
    "    \n",
    "    return num_lst, title_lst, url_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "83bbd80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_detail_info(url_lst, class_code):\n",
    "    proxy_function = free_proxy.get_proxy_list()\n",
    "    proxy_idx = 0\n",
    "    \n",
    "    def get_post_res(url, fun_p_idx):\n",
    "        while True:\n",
    "    \n",
    "            try:\n",
    "                proxies = {'http' : 'http://' + proxy_function[fun_p_idx][1], 'https' : 'http://' + proxy_function[fun_p_idx][1]}\n",
    "                headers = {\n",
    "                            'user-agent' : ua.random\n",
    "                            }\n",
    "\n",
    "\n",
    "                res = requests.get(url, headers = headers, proxies=proxies, timeout=(10,10))\n",
    "                \n",
    "                if res.status_code == 200:\n",
    "                    return res, fun_p_idx\n",
    "            except Exception as e:\n",
    "#                 print(e)\n",
    "                fun_p_idx += 1\n",
    "\n",
    "                if fun_p_idx == len(proxy_data):\n",
    "                    fun_p_idx = 0\n",
    "    \n",
    "    \n",
    "    base_url = 'https://www.inven.co.kr/board/lostark/'\n",
    "    post_detail = []\n",
    "    for url in tqdm(url_lst):\n",
    "        res, proxy_idx = get_post_res(url, proxy_idx)\n",
    "\n",
    "        soup = bs(res.text, 'html.parser')\n",
    "\n",
    "        # 내용\n",
    "        post_content = soup.select_one('#tbArticle > div.articleMain > div.articleContent > div #powerbbsContent')\n",
    "        if post_content:\n",
    "            post_content = post_content.text \n",
    "        else:\n",
    "            post_content = ''\n",
    "        \n",
    "        # 작성일자   \n",
    "        post_writed_at = soup.select_one('#tbArticle > div.articleHead.hC_silver1 > div > div.articleDate')\n",
    "        if post_writed_at:\n",
    "            post_writed_at = post_writed_at.text\n",
    "        else:\n",
    "            post_writed_at = ''\n",
    "        \n",
    "        articlecode = re.sub(base_url + f\"{class_code}/\" ,'',url)\n",
    "        \n",
    "#         print(articlecode)\n",
    "        comment_writed_at, comment_content, proxy_function, proxy_idx = get_post_comment(articlecode, proxy_function, proxy_idx, class_code)\n",
    "        \n",
    "        post_detail.append({'content' : post_content, 'writed_at' : post_writed_at, 'comments' : comment_content, 'comment_writed_at' : comment_writed_at})\n",
    "        \n",
    "    return post_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "79fdf316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_comment(articlecode , proxy_data, proxy_idx, class_code):\n",
    "    \n",
    "    comment_writed_at = []\n",
    "    comment_content = []\n",
    "    \n",
    "    comment_url = 'https://www.inven.co.kr/common/board/comment.json.php'\n",
    "    \n",
    "    while True:\n",
    "#         print(proxy_idx)\n",
    "        try:\n",
    "            proxies = {'http' : 'http://' + proxy_data[proxy_idx][1], 'https' : 'http://' + proxy_data[proxy_idx][1]}\n",
    "            headers = {\n",
    "            'User-Agent': ua.random,\n",
    "            'X-Requested-With':'XMLHttpRequest'\n",
    "            }\n",
    "\n",
    "            data={\n",
    "            'comeidx': str(class_code),\n",
    "            'articlecode': str(articlecode),\n",
    "            'sortorder': 'date',\n",
    "            'act': 'list',\n",
    "            'out': 'json',\n",
    "            'replyidx': '0',\n",
    "            }\n",
    "\n",
    "            cmt_res = requests.post(comment_url, headers = headers, data = data)\n",
    "\n",
    "            cmt_json = cmt_res.json()\n",
    "\n",
    "            if cmt_json['cmtcount']:\n",
    "                for cmt in cmt_res.json()['commentlist'][0]['list']:\n",
    "                    comment_writed_at.append(cmt['o_date'])\n",
    "                    content = (cmt['o_comment'])\n",
    "                    comment_content.append(re.sub('[&amp|nbsp|;]+', ' ', content))\n",
    "            else:\n",
    "                comment_content.append('')\n",
    "                comment_writed_at.append('')\n",
    "            break\n",
    "        except Exception as e:\n",
    "#             print(e)\n",
    "            proxy_idx += 1\n",
    "\n",
    "            if proxy_idx == len(proxy_data):\n",
    "                proxy_data = free_proxy.get_proxy_list()\n",
    "                proxy_idx = 0\n",
    "    \n",
    "    return comment_writed_at, comment_content, proxy_data, proxy_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "598ec70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_lst(num_lst, title_lst, url_lst, detail_info):\n",
    "\n",
    "\n",
    "    df_lst = []\n",
    "\n",
    "    for i in range(len(num_lst)):\n",
    "    #    num_lst, title_lst, url_lst, detail_info\n",
    "\n",
    "        # 'content', 'writed_at', 'comments', 'comment_writed_at'\n",
    "        details = detail_info[i]\n",
    "        content = details['content']\n",
    "        writed_at = details['writed_at']\n",
    "        comments = details['comments']\n",
    "        comment_writed_at = details['comment_writed_at']\n",
    "\n",
    "\n",
    "        df_lst.append(pd.DataFrame({\n",
    "            'num' : num_lst[i],\n",
    "            'title' : title_lst[i],\n",
    "            'content' : content,\n",
    "            'writed_at' : writed_at,\n",
    "            'comments' : [comments],\n",
    "            'comment_writed_at' : [comment_writed_at],\n",
    "            'url' : url_lst[i],\n",
    "        }))\n",
    "\n",
    "    return df_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7839514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_searched(lst, max_num, min_num):\n",
    "    \n",
    "    # 처음과 마지막 사이 글이라면 패스\n",
    "    go_crawl = False\n",
    "    for num in lst:\n",
    "        if (num > max_num) or (num < min_num):\n",
    "            go_crawl = True\n",
    "    \n",
    "    if not go_crawl :\n",
    "        return 0, 0\n",
    "    \n",
    "    # 크롤링 해야하는 최근 게시물\n",
    "    if max_num in lst:\n",
    "        last_idx = lst.index(max_num)\n",
    "    else:\n",
    "        last_idx = len(lst)\n",
    "    \n",
    "    # 크롤링 되지 않은 과거 게시물\n",
    "    if min_num in lst:\n",
    "        start_idx = lst.index(min_num)\n",
    "    else:\n",
    "        start_idx = 0\n",
    "        \n",
    "    return start_idx, last_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5c4ec265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df(df, df_lst):\n",
    "\n",
    "    add_df = pd.concat(df_lst).reset_index(drop=True)\n",
    "    \n",
    "    con_df = pd.concat([df, add_df], axis = 0).sort_values('num', ascending=False)\n",
    "\n",
    "        \n",
    "    con_df = con_df.drop_duplicates(subset=['title','content']).reset_index(drop=True)\n",
    "\n",
    "    return con_df\n",
    "\n",
    "def save_data(df, df_lst, class_code):\n",
    "    save_df = concat_df(df, df_lst)\n",
    "    save_df['classcode']  = class_code\n",
    "    \n",
    "    path = f'./data/{target_class}.csv'\n",
    "    \n",
    "    save_df.to_csv(path, encoding='utf8', index=False)\n",
    "    print(f'SAVE SUCCESS -> {path}')\n",
    "    \n",
    "def load_data(class_code = 0, className = ''):\n",
    "    \n",
    "    if class_code != 0:\n",
    "        name = code_to_name[class_code]\n",
    "    \n",
    "    elif className != '':\n",
    "        name = className\n",
    "    \n",
    "    try:\n",
    "        path = f'./data/{name}.csv'\n",
    "        df = pd.read_csv(path, encoding='utf8')\n",
    "    except:\n",
    "        print(f'Not Exist {name}.csv file')\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb1baf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = list(class_code.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0199a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCHING CLASS : 기상술사\n",
      "Crawling -> 1page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling -> 2page\n",
      "Passing Page\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Crawling -> 30page\n",
      "HTTPSConnectionPool(host='www.inven.co.kr', port=443): Max retries exceeded with url: /board/lostark/5862?p=30 (Caused by ProxyError('Unable to connect to proxy', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fb8ae5fcb90>: Failed to establish a new connection: [Errno 61] Connection refused')))\n",
      "Crawling -> 30page\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for target_class in targets[::-1]:\n",
    "    if target_class == '소울이터': continue\n",
    "    \n",
    "    print(f'SEARCHING CLASS : {target_class}')\n",
    "    \n",
    "    df = load_data(className = target_class)\n",
    "    \n",
    "    proxy_data = free_proxy.get_proxy_list()\n",
    "\n",
    "    articlecode = class_code[target_class]\n",
    "    test_url = class_url[class_code[target_class]]\n",
    "\n",
    "    if len(df):\n",
    "        searched_max_num = df.num.max()\n",
    "        searched_min_num = df.num.min()\n",
    "    else:\n",
    "        print('Maybe First Searching or Not Loaded Data')\n",
    "        searched_max_num, searched_min_num = 0, 0\n",
    "\n",
    "    p_idx = 0\n",
    "    page = 1\n",
    "    searching_flag = False\n",
    "\n",
    "    df_lst = []\n",
    "    while True:\n",
    "        print(f'Crawling -> {page}page')\n",
    "        try:\n",
    "            proxies = {'http' : 'http://' + proxy_data[p_idx][1], 'https' : 'http://' + proxy_data[p_idx][1]}\n",
    "            headers = {\n",
    "                        'user-agent' : ua.random\n",
    "                        }\n",
    "\n",
    "            params = {\n",
    "                'p' : page\n",
    "            }\n",
    "\n",
    "            res = requests.get(test_url, headers = headers, params=params, proxies=proxies, timeout=(10,10))\n",
    "\n",
    "            soup = bs(res.text, 'html.parser')\n",
    "\n",
    "#             print('get_post_basic_info start')\n",
    "            num_lst, title_lst, url_lst = get_post_basic_info(soup)\n",
    "#             print('get_post_basic_info fin')\n",
    "\n",
    "            num_lst = list(map(int, num_lst))\n",
    "\n",
    "            s_idx, l_idx = check_searched(num_lst, searched_max_num, searched_min_num)\n",
    "\n",
    "\n",
    "            num_lst = num_lst[s_idx : l_idx]\n",
    "            title_lst = title_lst[s_idx : l_idx]\n",
    "            url_lst = url_lst[s_idx : l_idx]\n",
    "\n",
    "\n",
    "            if not len(num_lst) : \n",
    "                print('Passing Page')\n",
    "                if page < 5:\n",
    "                    skip_page = int((searched_max_num-searched_min_num)/65)\n",
    "                    if skip_page < 5:\n",
    "                        page +=1\n",
    "                    else:\n",
    "                        page += skip_page\n",
    "                else:\n",
    "                    page+=1\n",
    "                continue\n",
    "            \n",
    "            end_date = '2023-05-12' # 밸런스패치 내용 공개일\n",
    "            page_last_article = ''\n",
    "            \n",
    "            # 페이지 첫 게시글/ 마지막 게시글 가져오기\n",
    "\n",
    "            page_articles = soup.select(f'#new-board > form > div > table > tbody > tr')\n",
    "#             print('articles cnt : ', len(page_articles))\n",
    "            for idx, at in enumerate(page_articles):\n",
    "#                 print(at['class'])\n",
    "                if 'notice' in at['class']:\n",
    "                    pass\n",
    "                else:\n",
    "                    page_articles = page_articles[idx:]\n",
    "                    break\n",
    "\n",
    "            page_frist_article = page_articles[1].select_one('td.date').text\n",
    "            last_idx = len(page_articles) -1\n",
    "                \n",
    "            try:                \n",
    "                while page_last_article == '':\n",
    "                    page_last_article = page_articles[last_idx].select_one('td.date').text\n",
    "                    \n",
    "            except:\n",
    "                article_last_idx -=1\n",
    "                \n",
    "\n",
    "            # 첫 게시글이 end_date보다 작으면 바로 중지\n",
    "            if ':' in page_frist_article: # 첫 게시글이 시간일 경우(1Page)\n",
    "                pass\n",
    "            else:\n",
    "                page_frist_article = '2023-' + page_frist_article\n",
    "                if page_frist_article < end_date:\n",
    "                    print('MODIFY END. 1st Article Date : {:15} / SET END DATE : {}'.format(page_frist_article,end_date))\n",
    "                    break\n",
    "\n",
    "            # 첫 게시글에서 중지가 안됐다면 마지막 게시글 확인하고 flag값 변경    \n",
    "            page_last_article = '2023-' + page_last_article\n",
    "            if page_last_article < end_date:\n",
    "                print('MODIFY END. Article Date : {:15} / SET END DATE : {}'.format(page_last_article,end_date))\n",
    "                searching_flag = True\n",
    "            \n",
    "#             print('get_post_detail_info start')\n",
    "            detail_info = get_post_detail_info(url_lst, articlecode)\n",
    "#             print('get_post_detail_info fin')\n",
    "\n",
    "            tmp = make_df_lst(num_lst, title_lst, url_lst, detail_info)\n",
    "\n",
    "            df_lst+=tmp\n",
    "            \n",
    "            # 페이지 서칭 종료\n",
    "            if searching_flag:\n",
    "                break\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            page+=1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "            p_idx += 1\n",
    "\n",
    "            if p_idx == len(proxy_data):\n",
    "                p_idx = 0\n",
    "    print(\"-\"*15)\n",
    "    print('END CLASS({}) ARTICLE IN INVEN'.format(target_class))\n",
    "    print(\"-\"*15)\n",
    "    \n",
    "    if len(df_lst):\n",
    "        save_data(df, df_lst, articlecode)\n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "58872d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.concat(df_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "98215f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-10 20:21\n"
     ]
    }
   ],
   "source": [
    "for a in t[-1:]['writed_at']:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d8a318b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "27d403a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = concat_df(df, df_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8c3a817d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE SUCCESS -> ./data/호크아이.csv\n"
     ]
    }
   ],
   "source": [
    "save_data(df, df_lst, target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "21ee71e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(className =target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3c5cb789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.492307692307692"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((searched_max_num-searched_min_num)/65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c6b952fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb2f296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "83b3dc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['notice', 'all']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_articles[0]['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f8ef32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(res.text, 'html.parser')\n",
    "page_articles=soup.select(f'#new-board > form > div > table > tbody > tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b78d0d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10-15'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_articles[0].select_one('td.date').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4feb0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://www.inven.co.kr/board/lostark/5343?p=1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_study",
   "language": "python",
   "name": "ds_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

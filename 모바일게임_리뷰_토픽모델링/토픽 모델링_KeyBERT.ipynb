{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 공통 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "# https://mr-doosun.tistory.com/24 Stopwords BASE & Custom\n",
    "\n",
    "with open('./data/stopwords_post_position.txt', 'r') as f:\n",
    "    josa_lst = f.readlines()\n",
    "\n",
    "with open('./data/stopword_conjunction.txt', 'r') as f:\n",
    "    conjunction_lst = f.readline().split(', ')\n",
    "\n",
    "# 불용어 처리\n",
    "stopwords_pPosition = []\n",
    "for josa in josa_lst:\n",
    "    josa = re.sub('\\n|\\t', '', josa)\n",
    "    if '/' in josa:\n",
    "        josa_words = josa.split('/')\n",
    "    else:\n",
    "        josa_words = [josa]\n",
    "\n",
    "    [stopwords_pPosition.append(word) for word in josa_words]\n",
    "\n",
    "def pp_stopwords_pposition(txt, stopwords = stopwords_pPosition):\n",
    "    \n",
    "    split_words = txt.split()\n",
    "\n",
    "    result = []\n",
    "    for word in split_words:\n",
    "        for length in range(max(map(len, stopwords)),0 , -1):\n",
    "            if word[-length:] in stopwords:\n",
    "                result.append(word[:-length])\n",
    "                break\n",
    "            elif length == 1:\n",
    "                result.append(word)\n",
    "\n",
    "    result = ' '.join(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def pp_stopwords_conjunction(txt, stopwords = conjunction_lst):\n",
    "    for stopword in stopwords:\n",
    "        if stopword in txt:\n",
    "\n",
    "            # Stopword의 위치 찾기\n",
    "            check_before_idx = re.search(stopword, txt).start() -1\n",
    "            check_after_idx = re.search(stopword, txt).end() # idx가 아니라 번째 개념으로 자동으로 +1 되어있음\n",
    "\n",
    "            # 시작위치가 첫번째일떄 예외처리\n",
    "            if check_before_idx == -1:\n",
    "                check_before_blank = True\n",
    "            else:\n",
    "                check_before_blank = True if txt[check_before_idx] == ' ' else False\n",
    "            \n",
    "            #종료지점이 끝위치일떄 예외처리\n",
    "            if check_after_idx == len(txt):\n",
    "                check_after_blank = True\n",
    "            else:\n",
    "                check_after_blank = True if txt[check_after_idx] == ' ' else False\n",
    "            \n",
    "            if check_before_blank and check_after_blank:\n",
    "                txt = re.sub(stopword, ' ', txt).strip()\n",
    "        \n",
    "    return txt\n",
    "\n",
    "def del_stopwords(txt):\n",
    "    txt = pp_stopwords_conjunction(txt) # 접속사 제거\n",
    "    txt = pp_stopwords_pposition(txt) # 조사 제거\n",
    "    txt = re.sub('[^가-힣]', ' ', txt).strip() # 한글 제외 제거\n",
    "    txt = repeat_normalize(txt, num_repeats=3)\n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/reivews_df_preprocssing_ver.csv')\n",
    "data['content'] = data['content'].apply(del_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from konlpy.tag import Okt, Mecab\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "b_a = data[data['app_name'] == '블루아카이브'].sample(n=6000).reset_index()\n",
    "n_k = data[data['app_name'] == '니케'].sample(n=6000).reset_index()\n",
    "o_g = data[data['app_name'] == '원신'].sample(n=6000).reset_index()\n",
    "d_s = data[data['app_name'] == '붕괴:스타레일'].sample(n=6000).reset_index()\n",
    "\n",
    "b_a_all = b_a['content']\n",
    "n_k_all = n_k['content']\n",
    "o_g_all = o_g['content']\n",
    "d_s_all = d_s['content']\n",
    "\n",
    "b_a_pos = b_a[b_a['score'] > 3]['content']\n",
    "b_a_na = n_k[n_k['score'] < 3]['content']\n",
    "n_k_pos = n_k[n_k['score'] > 3]['content']\n",
    "n_k_na = b_a[b_a['score'] < 3]['content']\n",
    "o_g_pos = o_g[o_g['score'] > 3]['content']\n",
    "o_g_na = o_g[o_g['score'] < 3]['content']\n",
    "d_s_pos = d_s[d_s['score'] > 3]['content']\n",
    "d_s_na = d_s[d_s['score'] < 3]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomKeyBERT():\n",
    "    def __init__(self, reviews, model):\n",
    "        self.reivews = reviews\n",
    "        self.model = SentenceTransformer(model)\n",
    "\n",
    "        self.okt = Okt()\n",
    "        self.mecab = Mecab()\n",
    "        \n",
    "        print('Preprocessing Start')\n",
    "        self.preprocessing()\n",
    "        print('Preprocessing Fin')\n",
    "\n",
    "    def preprocessing(self):\n",
    "        tokenized_okt = self.okt.pos(self.reivews)\n",
    "        tokenized_mecab = self.mecab.nouns(self.reivews)\n",
    "        tokenized_okt = ' '.join([word[0] for word in tokenized_okt if word[1] == 'Noun'])\n",
    "        tokenized_mecab = ' '.join(tokenized_mecab)\n",
    "\n",
    "\n",
    "        count_okt = CountVectorizer(ngram_range= (3,3)).fit([tokenized_okt])\n",
    "        count_mecab = CountVectorizer(ngram_range= (3,3)).fit([tokenized_mecab])\n",
    "        self.candidates1 = count_okt.get_feature_names_out()\n",
    "        self.candidates2 = count_mecab.get_feature_names_out()\n",
    "\n",
    "        self.doc_embedding = self.model.encode([self.reivews])\n",
    "        self.candidate_embeddings_okt = self.model.encode(self.candidates1)\n",
    "        self.candidate_embeddings_mecab = self.model.encode(self.candidates2)\n",
    "\n",
    "\n",
    "    def keyBert_Noraml_result(self, top_n):\n",
    "\n",
    "        distances_okt = cosine_similarity(self.doc_embedding, self.candidate_embeddings_okt)\n",
    "        distances_mecab = cosine_similarity(self.doc_embedding, self.candidate_embeddings_mecab)\n",
    "\n",
    "        keywords_okt = [self.candidates1[index] for index in distances_okt.argsort()[0][-top_n:]]\n",
    "        keywords_mecab = [self.candidates2[index] for index in distances_mecab.argsort()[0][-top_n:]]\n",
    "\n",
    "        return {'Okt' : keywords_okt, 'Mecab' : keywords_mecab}\n",
    "\n",
    "    def max_sum_sim(self, tagger, top_n, nr_candidates):\n",
    "\n",
    "        if tagger == 'Okt':\n",
    "            candidate_embeddings = self.candidate_embeddings_okt\n",
    "            words = self.candidates1\n",
    "\n",
    "        elif tagger == 'Mecab':\n",
    "            candidate_embeddings = self.candidate_embeddings_mecab\n",
    "            words = self.candidates2\n",
    "\n",
    "        else:\n",
    "            return '올바른 형태소 분석기를 선택하세요.(Okt, Mecab)'\n",
    "\n",
    "\n",
    "        # 문서와 각 키워드들 간의 유사도\n",
    "        distances = cosine_similarity(self.doc_embedding, candidate_embeddings)\n",
    "\n",
    "        # 각 키워드들 간의 유사도\n",
    "        distances_candidates = cosine_similarity(candidate_embeddings, candidate_embeddings)\n",
    "\n",
    "        words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "        words_vals = [words[index] for index in words_idx]\n",
    "        distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "        # 각 키워들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\n",
    "        min_sim = np.inf\n",
    "        candidate = None\n",
    "        for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "            sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "\n",
    "            if sim < min_sim:\n",
    "                candidate = combination\n",
    "                min_sim = sim\n",
    "\n",
    "        return [words_vals[idx] for idx in candidate]\n",
    "\n",
    "    def mmr(self, tagger, top_n, diversity):\n",
    "\n",
    "        if tagger == 'Okt':\n",
    "            candidate_embeddings = self.candidate_embeddings_okt\n",
    "            words = self.candidates1\n",
    "\n",
    "        elif tagger == 'Mecab':\n",
    "            candidate_embeddings = self.candidate_embeddings_mecab\n",
    "            words = self.candidates2\n",
    "        \n",
    "        else:\n",
    "            return '올바른 형태소 분석기를 선택하세요.(Okt, Mecab)'\n",
    "\n",
    "        # 문서와 각 키워드들 간의 유사도 리스트\n",
    "        word_doc_similarity = cosine_similarity(candidate_embeddings, self.doc_embedding)\n",
    "\n",
    "        # 키워간 유사도\n",
    "        word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "        # 문서와 가장 유사도가 높은 인덱스 추출\n",
    "        keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "\n",
    "        # 가장 높은 유사도 인덱스를 제외한 idx리스트\n",
    "        candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "        for _ in range(top_n-1):\n",
    "            candidate_similarites = word_doc_similarity[candidates_idx, :]\n",
    "            target_similarities = np.max(word_similarity[candidates_idx][:,keywords_idx], axis=1)\n",
    "\n",
    "            mmr = (1-diversity) * candidate_similarites - diversity * target_similarities.reshape(-1,1)\n",
    "            mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "            keywords_idx.append(mmr_idx)\n",
    "            candidates_idx.remove(mmr_idx)\n",
    "\n",
    "        return [words[idx] for idx in keywords_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomKeyBERT(리뷰 데이터, 모델이름)\n",
    "MODEL_NAME = 'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens'\n",
    "\n",
    "# 블루아카\n",
    "b_a_content_doc = ' '.join(list(b_a_all.values))\n",
    "b_a_content_pos_doc = ' '.join(list(b_a_pos.values))\n",
    "b_a_content_na_doc = ' '.join(list(b_a_na.values))\n",
    "\n",
    "b_a_KeyBERT = CustomKeyBERT(b_a_content_doc, MODEL_NAME)\n",
    "b_a_KeyBERT_pos = CustomKeyBERT(b_a_content_pos_doc, MODEL_NAME)\n",
    "b_a_KeyBERT_na = CustomKeyBERT(b_a_content_na_doc, MODEL_NAME)\n",
    "\n",
    "print('코사인 유사도 - 전체')\n",
    "print(b_a_KeyBERT.keyBert_Noraml_result(5))\n",
    "print()\n",
    "print('MSS(Mecab)')\n",
    "print(b_a_KeyBERT.max_sum_sim('Mecab', 5, 10))\n",
    "print('MSS(Okt)')\n",
    "print(b_a_KeyBERT.max_sum_sim('Okt', 5, 10))\n",
    "print()\n",
    "print('MMR(Mecab)')\n",
    "print(b_a_KeyBERT.mmr('Mecab', 5, 0.7))\n",
    "print('MMR(Okt)')\n",
    "print(b_a_KeyBERT.mmr('Okt', 5, 0.7))\n",
    "\n",
    "\n",
    "print()\n",
    "print('코사인 유사도 - 긍정')\n",
    "print(b_a_KeyBERT_pos.keyBert_Noraml_result(5))\n",
    "print()\n",
    "print('MSS(Mecab)')\n",
    "print(b_a_KeyBERT_pos.max_sum_sim('Mecab', 5, 10))\n",
    "print('MSS(Okt)')\n",
    "print(b_a_KeyBERT_pos.max_sum_sim('Okt', 5, 10))\n",
    "print()\n",
    "print('MMR(Mecab)')\n",
    "print(b_a_KeyBERT_pos.mmr('Mecab', 5, 0.7))\n",
    "print('MMR(Okt)')\n",
    "print(b_a_KeyBERT_pos.mmr('Okt', 5, 0.7))\n",
    "\n",
    "print()\n",
    "print('코사인 유사도 - 부정')\n",
    "print(b_a_KeyBERT_na.keyBert_Noraml_result(5))\n",
    "print()\n",
    "print('MSS(Mecab)')\n",
    "print(b_a_KeyBERT_na.max_sum_sim('Mecab', 5, 10))\n",
    "print('MSS(Okt)')\n",
    "print(b_a_KeyBERT_na.max_sum_sim('Okt', 5, 10))\n",
    "print()\n",
    "print('MMR(Mecab)')\n",
    "print(b_a_KeyBERT_na.mmr('Mecab', 5, 0.7))\n",
    "print('MMR(Okt)')\n",
    "print(b_a_KeyBERT_na.mmr('Okt', 5, 0.7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomKeyBERT(리뷰 데이터, 모델이름)\n",
    "MODEL_NAME = 'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens'\n",
    "\n",
    "# 블루아카\n",
    "content_doc = ' '.join(list(n_k_all.values))\n",
    "content_pos_doc = ' '.join(list(n_k_pos.values))\n",
    "content_na_doc = ' '.join(list(n_k_na.values))\n",
    "\n",
    "KeyBERT = CustomKeyBERT(content_doc, MODEL_NAME)\n",
    "KeyBERT_pos = CustomKeyBERT(content_pos_doc, MODEL_NAME)\n",
    "KeyBERT_na = CustomKeyBERT(content_na_doc, MODEL_NAME)\n",
    "\n",
    "print('코사인 유사도 - 전체')\n",
    "print(KeyBERT.keyBert_Noraml_result(5))\n",
    "print()\n",
    "print('MSS(Mecab)')\n",
    "print(KeyBERT.max_sum_sim('Mecab', 5, 10))\n",
    "print('MSS(Okt)')\n",
    "print(KeyBERT.max_sum_sim('Okt', 5, 10))\n",
    "print()\n",
    "print('MMR(Mecab)')\n",
    "print(KeyBERT.mmr('Mecab', 5, 0.7))\n",
    "print('MMR(Okt)')\n",
    "print(KeyBERT.mmr('Okt', 5, 0.7))\n",
    "\n",
    "\n",
    "print()\n",
    "print('코사인 유사도 - 긍정')\n",
    "print(KeyBERT_pos.keyBert_Noraml_result(5))\n",
    "print()\n",
    "print('MSS(Mecab)')\n",
    "print(KeyBERT_pos.max_sum_sim('Mecab', 5, 10))\n",
    "print('MSS(Okt)')\n",
    "print(KeyBERT_pos.max_sum_sim('Okt', 5, 10))\n",
    "print()\n",
    "print('MMR(Mecab)')\n",
    "print(KeyBERT_pos.mmr('Mecab', 5, 0.7))\n",
    "print('MMR(Okt)')\n",
    "print(KeyBERT_pos.mmr('Okt', 5, 0.7))\n",
    "\n",
    "print()\n",
    "print('코사인 유사도 - 부정')\n",
    "print(KeyBERT_na.keyBert_Noraml_result(5))\n",
    "print()\n",
    "print('MSS(Mecab)')\n",
    "print(KeyBERT_na.max_sum_sim('Mecab', 5, 10))\n",
    "print('MSS(Okt)')\n",
    "print(KeyBERT_na.max_sum_sim('Okt', 5, 10))\n",
    "print()\n",
    "print('MMR(Mecab)')\n",
    "print(KeyBERT_na.mmr('Mecab', 5, 0.7))\n",
    "print('MMR(Okt)')\n",
    "print(KeyBERT_na.mmr('Okt', 5, 0.7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomKeyBERT(리뷰 데이터, 모델이름)\n",
    "MODEL_NAME = 'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens'\n",
    "\n",
    "# 블루아카\n",
    "content_doc = ' '.join(list(o_g_all.values))\n",
    "content_pos_doc = ' '.join(list(o_g_pos.values))\n",
    "content_na_doc = ' '.join(list(o_g_na.values))\n",
    "\n",
    "KeyBERT = CustomKeyBERT(content_doc, MODEL_NAME)\n",
    "KeyBERT_pos = CustomKeyBERT(content_pos_doc, MODEL_NAME)\n",
    "KeyBERT_na = CustomKeyBERT(content_na_doc, MODEL_NAME)\n",
    "\n",
    "print('코사인 유사도 - 전체')\n",
    "print(KeyBERT.keyBert_Noraml_result(5))\n",
    "print()\n",
    "print('MSS(Mecab)')\n",
    "print(KeyBERT.max_sum_sim('Mecab', 5, 10))\n",
    "print('MSS(Okt)')\n",
    "print(KeyBERT.max_sum_sim('Okt', 5, 10))\n",
    "print()\n",
    "print('MMR(Mecab)')\n",
    "print(KeyBERT.mmr('Mecab', 5, 0.7))\n",
    "print('MMR(Okt)')\n",
    "print(KeyBERT.mmr('Okt', 5, 0.7))\n",
    "\n",
    "\n",
    "print()\n",
    "print('코사인 유사도 - 긍정')\n",
    "print(KeyBERT_pos.keyBert_Noraml_result(5))\n",
    "print()\n",
    "print('MSS(Mecab)')\n",
    "print(KeyBERT_pos.max_sum_sim('Mecab', 5, 10))\n",
    "print('MSS(Okt)')\n",
    "print(KeyBERT_pos.max_sum_sim('Okt', 5, 10))\n",
    "print()\n",
    "print('MMR(Mecab)')\n",
    "print(KeyBERT_pos.mmr('Mecab', 5, 0.7))\n",
    "print('MMR(Okt)')\n",
    "print(KeyBERT_pos.mmr('Okt', 5, 0.7))\n",
    "\n",
    "print()\n",
    "print('코사인 유사도 - 부정')\n",
    "print(KeyBERT_na.keyBert_Noraml_result(5))\n",
    "print()\n",
    "print('MSS(Mecab)')\n",
    "print(KeyBERT_na.max_sum_sim('Mecab', 5, 10))\n",
    "print('MSS(Okt)')\n",
    "print(KeyBERT_na.max_sum_sim('Okt', 5, 10))\n",
    "print()\n",
    "print('MMR(Mecab)')\n",
    "print(KeyBERT_na.mmr('Mecab', 5, 0.7))\n",
    "print('MMR(Okt)')\n",
    "print(KeyBERT_na.mmr('Okt', 5, 0.7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomKeyBERT(리뷰 데이터, 모델이름)\n",
    "MODEL_NAME = 'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens'\n",
    "\n",
    "# 블루아카\n",
    "content_doc = ' '.join(list(d_s_all.values))\n",
    "content_pos_doc = ' '.join(list(d_s_pos.values))\n",
    "content_na_doc = ' '.join(list(d_s_na.values))\n",
    "\n",
    "KeyBERT = CustomKeyBERT(content_doc, MODEL_NAME)\n",
    "KeyBERT_pos = CustomKeyBERT(content_pos_doc, MODEL_NAME)\n",
    "KeyBERT_na = CustomKeyBERT(content_na_doc, MODEL_NAME)\n",
    "\n",
    "print('코사인 유사도 - 전체')\n",
    "print(KeyBERT.keyBert_Noraml_result(5))\n",
    "print()\n",
    "print('MSS(Mecab)')\n",
    "print(KeyBERT.max_sum_sim('Mecab', 5, 10))\n",
    "print('MSS(Okt)')\n",
    "print(KeyBERT.max_sum_sim('Okt', 5, 10))\n",
    "print()\n",
    "print('MMR(Mecab)')\n",
    "print(KeyBERT.mmr('Mecab', 5, 0.7))\n",
    "print('MMR(Okt)')\n",
    "print(KeyBERT.mmr('Okt', 5, 0.7))\n",
    "\n",
    "\n",
    "print()\n",
    "print('코사인 유사도 - 긍정')\n",
    "print(KeyBERT_pos.keyBert_Noraml_result(5))\n",
    "print()\n",
    "print('MSS(Mecab)')\n",
    "print(KeyBERT_pos.max_sum_sim('Mecab', 5, 10))\n",
    "print('MSS(Okt)')\n",
    "print(KeyBERT_pos.max_sum_sim('Okt', 5, 10))\n",
    "print()\n",
    "print('MMR(Mecab)')\n",
    "print(KeyBERT_pos.mmr('Mecab', 5, 0.7))\n",
    "print('MMR(Okt)')\n",
    "print(KeyBERT_pos.mmr('Okt', 5, 0.7))\n",
    "\n",
    "print()\n",
    "print('코사인 유사도 - 부정')\n",
    "print(KeyBERT_na.keyBert_Noraml_result(5))\n",
    "print()\n",
    "print('MSS(Mecab)')\n",
    "print(KeyBERT_na.max_sum_sim('Mecab', 5, 10))\n",
    "print('MSS(Okt)')\n",
    "print(KeyBERT_na.max_sum_sim('Okt', 5, 10))\n",
    "print()\n",
    "print('MMR(Mecab)')\n",
    "print(KeyBERT_na.mmr('Mecab', 5, 0.7))\n",
    "print('MMR(Okt)')\n",
    "print(KeyBERT_na.mmr('Okt', 5, 0.7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b6a945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent(verify_ssl=False)\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f15b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'user-agent' : ua.random\n",
    "}\n",
    "base_url = 'https://df.nexon.com/df/community/dnfboard'\n",
    "params = {\n",
    "    'mode': 'list',\n",
    "    'order': 'reg_date',\n",
    "    'order_type': 'DESC',\n",
    "    'job' : '99', # 게시판 코드 99-> 공통\n",
    "    \"grow_type\": '0',\n",
    "    'page' : '0' # 페이지 설정\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "695efd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230515\n",
      "./data/offical_page_20230515.csv\n",
      "File exist Load FIN!!\n",
      "20230511\n",
      "./data/offical_page_20230511.csv\n",
      "File exist Load FIN!!\n",
      "20230512\n",
      "./data/offical_page_20230512.csv\n",
      "File exist Load FIN!!\n",
      "20230513\n",
      "./data/offical_page_20230513.csv\n",
      "SEARCH POST\n",
      "request Setting FIN\n",
      "14 13 PASSING\n",
      "request Setting FIN\n",
      "HTTPSConnectionPool(host='df.nexon.com', port=443): Max retries exceeded with url: /df/community/dnfboard?mode=list&order=reg_date&order_type=DESC&job=99&grow_type=0&page=31 (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out')))\n",
      "request Setting FIN\n",
      "14 13 PASSING\n",
      "request Setting FIN\n",
      "14 13 PASSING\n",
      "request Setting FIN\n",
      "14 13 PASSING\n",
      "request Setting FIN\n",
      "GET POST\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "34\n",
      "request Setting FIN\n",
      "GET POST\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "35\n",
      "request Setting FIN\n",
      "GET POST\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "36\n",
      "request Setting FIN\n",
      "GET POST\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "37\n",
      "request Setting FIN\n",
      "GET POST\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "38\n",
      "request Setting FIN\n",
      "GET POST\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "39\n",
      "request Setting FIN\n",
      "GET POST\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "40\n",
      "request Setting FIN\n",
      "GET POST\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "41\n",
      "request Setting FIN\n",
      "GET POST\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "42\n",
      "request Setting FIN\n",
      "GET POST\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "43\n",
      "request Setting FIN\n",
      "GET POST\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "신고에 의해 숨김 처리된 게시물입니다.\n",
      "44\n",
      "request Setting FIN\n",
      "GET POST\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "45\n",
      "request Setting FIN\n",
      "HTTPSConnectionPool(host='df.nexon.com', port=443): Max retries exceeded with url: /df/community/dnfboard?mode=list&order=reg_date&order_type=DESC&job=99&grow_type=0&page=46 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)')))\n",
      "request Setting FIN\n",
      "HTTPSConnectionPool(host='df.nexon.com', port=443): Max retries exceeded with url: /df/community/dnfboard?mode=list&order=reg_date&order_type=DESC&job=99&grow_type=0&page=46 (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out')))\n",
      "request Setting FIN\n",
      "HTTPSConnectionPool(host='df.nexon.com', port=443): Max retries exceeded with url: /df/community/dnfboard?mode=list&order=reg_date&order_type=DESC&job=99&grow_type=0&page=46 (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(54, 'Connection reset by peer')))\n",
      "request Setting FIN\n",
      "HTTPSConnectionPool(host='df.nexon.com', port=443): Max retries exceeded with url: /df/community/dnfboard?mode=list&order=reg_date&order_type=DESC&job=99&grow_type=0&page=46 (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out')))\n",
      "request Setting FIN\n",
      "HTTPSConnectionPool(host='df.nexon.com', port=443): Max retries exceeded with url: /df/community/dnfboard?mode=list&order=reg_date&order_type=DESC&job=99&grow_type=0&page=46 (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(54, 'Connection reset by peer')))\n",
      "request Setting FIN\n",
      "HTTPSConnectionPool(host='df.nexon.com', port=443): Max retries exceeded with url: /df/community/dnfboard?mode=list&order=reg_date&order_type=DESC&job=99&grow_type=0&page=46 (Caused by ProxyError('Cannot connect to proxy.', timeout('timed out')))\n",
      "request Setting FIN\n",
      "GET POST\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "PASSING NOTICE\n",
      "break 2023-05-12 00:00:00\n",
      "SEARCH FIN! BREAK ROOP\n",
      "46\n",
      "Frist Crawling\n"
     ]
    }
   ],
   "source": [
    "# 검색할때 설정해줘야할 것들\n",
    "for target_date in dates:\n",
    "    path = './data/offical_page'\n",
    "\n",
    "    df = load_data(path, target_date)\n",
    "\n",
    "    if not len(df):\n",
    "        is_concat = False\n",
    "\n",
    "    else:\n",
    "        is_concat = True\n",
    "        continue\n",
    "\n",
    "    # dc_id = \"dnfqq\"\n",
    "\n",
    "    # if len(df) == 0:\n",
    "    #     y_day = datetime.today() - timedelta(days=1)\n",
    "    #     y_day_string = str(y_day.year) + \"-\" + str(y_day.month) + \"-\" + str(y_day.day)\n",
    "    #     last_timestamp = pd.to_datetime(y_day_string)\n",
    "    # else:\n",
    "    #     last_timestamp = pd.to_datetime(df.sort_values('timestamp', ascending=False)['timestamp'].iloc[0])\n",
    "\n",
    "    # print(last_timestamp)\n",
    "\n",
    "    search_data = search_post(pd.to_datetime(target_date))\n",
    "\n",
    "    if is_concat:\n",
    "        print('Concat New Post')\n",
    "        df = pd.concat([pd.DataFrame(search_data), df])\n",
    "    else:\n",
    "        print('Frist Crawling')\n",
    "        df = pd.DataFrame(search_data)\n",
    "\n",
    "    save_daily_data(df, path, target_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a91f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post(soup, target_date):\n",
    "    print('GET POST')\n",
    "    base_url = 'https://df.nexon.com/df/community/dnfboard'\n",
    "    break_flag = False\n",
    "    posts = soup.select('table.tbl > tbody > tr')\n",
    "    \n",
    "    cols = ['title', 'writed_at', 'refresh', 'recommand', 'link']\n",
    "    \n",
    "    post_dict = {key : [] for key in cols}\n",
    "    \n",
    "    notice_cnt = 0\n",
    "    \n",
    "    block_text = '신고에 의해 숨김 처리된 게시물입니다.'\n",
    "    \n",
    "    for post in posts:\n",
    "        try:\n",
    "            if post['class']: \n",
    "                print('PASSING NOTICE')\n",
    "                notice_cnt+=1\n",
    "                continue\n",
    "        except:\n",
    "            pass # 공지가 아닌 글들은 class가 없어서 오류 발생\n",
    "        \n",
    "        if post.select('td'):\n",
    "            \n",
    "            # 신고 처리된 게시물일 경우 패스\n",
    "            if post.select_one('td.tit > strong'):\n",
    "                print(post.select_one('td.tit > strong').text)\n",
    "                notice_cnt+=1\n",
    "                continue\n",
    "            \n",
    "            writed_at = post.select('td')[2].text\n",
    "            \n",
    "            # 검색 날짜보다 이전 결과라면 탈출\n",
    "            if pd.to_datetime('2023-' + writed_at).day < target_date.day:\n",
    "                print('break', pd.to_datetime('2023-' + writed_at))\n",
    "                break\n",
    "            \n",
    "            post_info = post.select_one('td.tit > a')\n",
    "            title = post_info['title']\n",
    "            link = base_url + post_info['href']\n",
    "#             title = re.sub('[\\r\\n]+', '' , title).strip()\n",
    "\n",
    "\n",
    "            refresh = post.select('td')[3].text\n",
    "            recommand = post.select('td')[4].text\n",
    "\n",
    "            data = [title, writed_at, refresh, recommand, link]\n",
    "            \n",
    "            for key, value in zip(cols, data):\n",
    "                post_dict[key].append(value)\n",
    "\n",
    "    \n",
    "    if len(posts) -1 - notice_cnt  > len(post_dict['title']):\n",
    "        print('SEARCH FIN! BREAK ROOP')\n",
    "        break_flag = True\n",
    "    \n",
    "    return pd.DataFrame(post_dict), break_flag\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffcfc853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_post(target_date):\n",
    "    print('SEARCH POST')\n",
    "    \n",
    "    df_lst = []\n",
    "    \n",
    "    headers = {\n",
    "        'user-agent' : ua.random\n",
    "    }\n",
    "    url = 'https://df.nexon.com/df/community/dnfboard'\n",
    "    params = {\n",
    "        'mode': 'list',\n",
    "        'order': 'reg_date',\n",
    "        'order_type': 'DESC',\n",
    "        'job' : '99', # 게시판 코드 99-> 공통\n",
    "        \"grow_type\": '0',\n",
    "    #     'page' : '1' # 페이지 설정\n",
    "        \n",
    "    }\n",
    "    \n",
    "    proxy_data = get_proxy_list()\n",
    "    \n",
    "    proxy_idx = 0\n",
    "    break_flag = False\n",
    "    page = 30\n",
    "        \n",
    "    while True:\n",
    "        try:\n",
    "            params['page'] = str(page)\n",
    "            proxies = {'http' : 'http://' + proxy_data[proxy_idx][1], 'https' : 'http://' + proxy_data[proxy_idx][1]}\n",
    "            headers = {\n",
    "                'user-agent' : ua.random\n",
    "            }\n",
    "            \n",
    "            print('request Setting FIN')\n",
    "            \n",
    "            res = requests.get(url, headers = headers, proxies = proxies, params=params, timeout=(10,10))\n",
    "            \n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            \n",
    "            last_post_writed_at = soup.select('table.tbl > tbody > tr')[-1].select('td')[2].text\n",
    "            \n",
    "            # 내가 찾고자 하는 날짜보다 최근이라면 다음페이지로 이동\n",
    "            if int(last_post_writed_at[-2:]) > target_date.day:\n",
    "                print(last_post_writed_at[-2:], target_date.day, \"PASSING\")\n",
    "                page+=1\n",
    "                continue\n",
    "            \n",
    "            df, break_flag = get_post(soup, target_date)\n",
    "            print(page)\n",
    "            df_lst.append(df)\n",
    "            \n",
    "            \n",
    "            if break_flag:\n",
    "                break\n",
    "            \n",
    "            page+=1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            headers = {'user-agent' : ua.random}\n",
    "            proxy_idx +=1\n",
    "            if proxy_idx >= len(proxy_data):\n",
    "                print('REFRESH PROXY AND RESET IDX')\n",
    "                proxy_data = get_proxy_list()\n",
    "                proxy_idx = 0\n",
    "        \n",
    "    return  pd.concat(df_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c301509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, load_date = -1): # 찾으려는 날짜를 특정하지 않으면 어제 게시글 탐색\n",
    "    y_day = datetime.today() - timedelta(days=1)\n",
    "    \n",
    "    if load_date == -1:\n",
    "    \n",
    "        load_date = datetime(y_day.year, y_day.month, y_day.day).strftime('%y%m%d')\n",
    "    \n",
    "    print(load_date)\n",
    "\n",
    "    \n",
    "    path = f'{file_path}_{load_date}.csv'\n",
    "    \n",
    "    print(path)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print('File exist Load FIN!!')\n",
    "    except:\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11229db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_daily_data(df, file_path, load_date = -1): # 저장하는 날짜를 특정하지 않으면 어제 게시글로 저장\n",
    "    y_day = datetime.today() - timedelta(days=1)\n",
    "    \n",
    "    if load_date == -1:\n",
    "        load_date = datetime(y_day.year, y_day.month, y_day.day).strftime('%y%m%d')\n",
    "    \n",
    "    path = f'{file_path}_{load_date}.csv'\n",
    "    \n",
    "    df.to_csv(path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d46280f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proxy_list():\n",
    "    url = 'http://spys.one/en/free-proxy-list/'\n",
    "    data = {\n",
    "        'xpp' : '1',\n",
    "        'xf1' : '0',\n",
    "        'xf2' : '0',\n",
    "        'xf4' : '0',\n",
    "        'xf5' : '2'\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    r = requests.post(url, data=data, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "    result = []\n",
    "\n",
    "    ports = {}\n",
    "    script = soup.select_one('body > script')\n",
    "    for row in script.text.split(';'):\n",
    "        if '^' in row:\n",
    "            line = row.split('=')\n",
    "            ports[line[0]]  = line[1].split('^')[0]\n",
    "\n",
    "    trs = soup.select('tr[onmouseover]')\n",
    "    for tr in trs:\n",
    "\n",
    "        is_http = tr.select_one('a')\n",
    "\n",
    "        if is_http is not None:\n",
    "            http = is_http.text\n",
    "#             print(is_http.text)\n",
    "\n",
    "        e_ip = tr.select_one('font.spy14')\n",
    "#         print(e_ip)\n",
    "        ip = ''\n",
    "\n",
    "        e_port = tr.select_one('script')\n",
    "        port = ''\n",
    "        if e_port is not None:\n",
    "            re_port = re.compile(r'\\(([a-zA-Z0-9]+)\\^[a-zA-Z0-9]+\\)')\n",
    "            match = re_port.findall(e_port.text)\n",
    "            for item in match:\n",
    "                port = port + ports[item]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if e_ip is not None:\n",
    "            for item in e_ip.findAll('script'):\n",
    "                item.extract()\n",
    "            ip = e_ip.text\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        tds = tr.select(\"td\")\n",
    "        is_skip = False\n",
    "        for td in tds:\n",
    "            e_pct = td.select_one('font > acronym')\n",
    "            if e_pct is not None:\n",
    "                pct = re.sub('([0-9]+)%.*', r'\\1', e_pct.text)\n",
    "                if not pct.isdigit():\n",
    "                    is_skip = True\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        if not pct.isdigit():\n",
    "            continue\n",
    "\n",
    "        result.append((http,ip + ':' + port, pct))\n",
    "\n",
    "    result.sort(key = lambda element  : int(element[2]), reverse=True)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41c717b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-05-11 00:00:00')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6188d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['20230515','20230511', '20230512', '20230513']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "309e0cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230511\n",
      "./data/offical_page_20230511.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m\n\u001b[1;32m     10\u001b[0m     is_concat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# dc_id = \"dnfqq\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# if len(df) == 0:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(last_timestamp)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m search_data \u001b[38;5;241m=\u001b[39m \u001b[43msearch_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_date\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_concat:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConcat New Post\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36msearch_post\u001b[0;34m(target_date)\u001b[0m\n\u001b[1;32m     28\u001b[0m proxies \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m proxy_data[proxy_idx][\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m proxy_data[proxy_idx][\u001b[38;5;241m1\u001b[39m]}\n\u001b[1;32m     29\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m'\u001b[39m : ua\u001b[38;5;241m.\u001b[39mrandom\n\u001b[1;32m     31\u001b[0m }\n\u001b[0;32m---> 33\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(res\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m last_post_writed_at \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable.tbl > tbody > tr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/sessions.py:745\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 745\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/response.py:623\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 623\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content):\n\u001b[1;32m    624\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m line\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/response.py:815\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/response.py:745\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    746\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1241\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 검색할때 설정해줘야할 것들\n",
    "for target_date in dates:\n",
    "    path = './data/offical_page'\n",
    "\n",
    "    df = load_data(path, target_date)\n",
    "\n",
    "    if not len(df):\n",
    "        is_concat = False\n",
    "    else:\n",
    "        is_concat = True\n",
    "\n",
    "    # dc_id = \"dnfqq\"\n",
    "\n",
    "    # if len(df) == 0:\n",
    "    #     y_day = datetime.today() - timedelta(days=1)\n",
    "    #     y_day_string = str(y_day.year) + \"-\" + str(y_day.month) + \"-\" + str(y_day.day)\n",
    "    #     last_timestamp = pd.to_datetime(y_day_string)\n",
    "    # else:\n",
    "    #     last_timestamp = pd.to_datetime(df.sort_values('timestamp', ascending=False)['timestamp'].iloc[0])\n",
    "\n",
    "    # print(last_timestamp)\n",
    "\n",
    "    search_data = search_post(pd.to_datetime(target_date))\n",
    "\n",
    "    if is_concat:\n",
    "        print('Concat New Post')\n",
    "        df = pd.concat([pd.DataFrame(search_data), df])\n",
    "    else:\n",
    "        print('Frist Crawling')\n",
    "        df = pd.DataFrame(search_data)\n",
    "\n",
    "    save_daily_data(df, path, target_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2390fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

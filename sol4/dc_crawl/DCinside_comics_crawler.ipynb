{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/seokholee/lsh/Project/sol4/dc_crawl\n"
     ]
    }
   ],
   "source": [
    "# Scrapy 시작할 폴더\n",
    "%cd /Users/seokholee/lsh/Project/sol4/dc_crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapy Project Create\n",
    "!scrapy startproject dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install fake-useragent\n",
    "!pip install scrapy-fake-useragent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. items.py 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dc/dc/items.py\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class DcItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    content = scrapy.Field()\n",
    "    date = scrapy.Field()\n",
    "    views = scrapy.Field()\n",
    "    recommend = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    gall_code = scrapy.Field()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. spider 만들기\n",
    "- time:sleep(1) 로 크롤링 차단 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile dc/dc/spiders/spider.py\n",
    "\n",
    "import scrapy\n",
    "import time\n",
    "from dc.items import DcItem\n",
    "from dc.custom_fun import *\n",
    "\n",
    "class DcSpider(scrapy.Spider):\n",
    "    name = \"dc\"\n",
    "    custom_settings = {\n",
    "        'DOWNLOADER_MIDDLEWARES' : {\n",
    "            'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n",
    "            'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        with open('/Users/seokholee/lsh/Project/sol4/dc_crawl/dc/dc/spiders/target_gall.txt', 'r') as f:\n",
    "            gall_ids = f.readlines()\n",
    "        \n",
    "        self.url_lst = []\n",
    "        for gall_id in gall_ids:\n",
    "            gall_id = re.sub('\\n', '', gall_id)\n",
    "            self.url_lst.append(make_galleryURL(gall_id))\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def start_requests(self):\n",
    "\n",
    "        # 10페이지 클롤링 / 페이지당 게시글 100개 총 1000개의 게시글 크롤링\n",
    "        for url in self.url_lst:\n",
    "            print('START CRAWL : ' + url)\n",
    "            for page in range(1,2):\n",
    "                gall_url = url + \"&page=\" + str(page) + \"&list_num=100\"\n",
    "                yield scrapy.Request(gall_url, callback=self.parse, meta = { 'url' : url })\n",
    "        \n",
    "    def parse(self, response):\n",
    "        links = response.xpath('//*[@class=\"ub-content us-post\"]/td/a/@href').extract()\n",
    "        links = list(map(response.urljoin, links))\n",
    "        print('RESPONSE URLJOIN : ', links)\n",
    "        for link in links:\n",
    "            yield scrapy.Request(link, callback=self.page_parse, meta={**response.meta})\n",
    "    \n",
    "    def page_parse(self, response):\n",
    "        item = DcItem()\n",
    "        item[\"title\"] = response.xpath('//*[@class=\"gallview_head clear ub-content\"]/h3/span[2]/text()').extract_first()\n",
    "        \n",
    "        contents = response.xpath('//*[@class=\"write_div\"]/p/text()').extract()\n",
    "        post = ''\n",
    "        for content in contents:\n",
    "            content = re.sub('[\\n\\t]', '', content)\n",
    "            content += '\\n'\n",
    "            post += content\n",
    "        item['content'] = post[:-2]\n",
    "\n",
    "        item[\"date\"] = response.xpath('//*[@class=\"gall_date\"]/text()').extract_first()\n",
    "        item[\"views\"] = response.xpath('//*[@class=\"fr\"]/span[1]/text()').extract_first()[3:]\n",
    "        item[\"recommend\"] = response.xpath('//*[@class=\"gall_reply_num\"]/text()').extract_first()[3:]\n",
    "        item[\"link\"] = response.url\n",
    "        \n",
    "        url = response.meta.pop('url')\n",
    "        item['gall_code'] = re.search('(?<=id=).+', url).group(0)\n",
    "        \n",
    "        yield item\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. setting.py 변경\n",
    "- ROBOTSTXT_OBEY = False\n",
    "- ITEM_PIPELINES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dc/dc/settings.py\n",
    "\n",
    "\n",
    "# Scrapy settings for comics project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'dc'\n",
    "\n",
    "SPIDER_MODULES = ['dc.spiders']\n",
    "NEWSPIDER_MODULE = 'dc.spiders'\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "#USER_AGENT = 'comics (+http://www.yourdomain.com)'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "# 다운로드 딜레이\n",
    "DOWNLOAD_DELAY = 0.5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 갤러리 입력\n",
    "\n",
    "- dc인사이드 접속 -> 원하는 개럴리 접속\n",
    "- 주소 중 ?id = \" \" <- id= 뒤에있는 코드 입력 \n",
    "- 한줄에 갤러리 코드 하나씩 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dc/dc/spiders/target_gall.txt\n"
     ]
    }
   ],
   "source": [
    "football_new8\n",
    "dnfqq\n",
    "comic_new3\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 종료시간 입력\n",
    "- 어느 날짜까지 크롤링할지 입력\n",
    "- YYYY-mm-dd HH:MM:SS 형식 유지 (DC인사이드 시간 정보 포맷)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dc/dc/spiders/target_gall.txt\n",
    "2023-08-08 15:09:33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/seokholee/lsh/Project/sol4/dc_crawl/dc/dc/spiders\n"
     ]
    }
   ],
   "source": [
    "# 이 명령어를 실행하면 위 코드 블록은 경로가 달라져서 실행이 안됨.\n",
    "# Scrapy 실행을 위해 클롤러 위치로 이동\n",
    "%cd /Users/seokholee/lsh/Project/sol4/dc_crawl/dc/dc/spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤러 실행 -o 저장할 파일이름.csv\n",
    "!scrapy crawl dc -o dc_crawl.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
